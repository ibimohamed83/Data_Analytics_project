{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7aeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca252809",
   "metadata": {},
   "source": [
    "Below code is used to clean the data. Below steps are done\n",
    "1. Read the CSV file from guthub and describe it\n",
    "2. Identify the timestamp column, convert to datetime index, verify 10-minute interval\n",
    "3. Remove any exact duplicate rows\n",
    "4. Handle missing values and if found take a mean of it\n",
    "5. Check for gaps any missing timestamps\n",
    "6. Fill Categorical value with NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae74daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV URL from gihub\n",
    "RAW_URL = \"https://raw.githubusercontent.com/ibimohamed83/Data_Analytics_project/refs/heads/main/dataset/energydata_complete.csv\"\n",
    "# Defining column name\n",
    "DATE_COL = \"date\"\n",
    "EXPECTED_FREQ = \"10min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (rows, cols): (19735, 29)\n",
      "\n",
      "Columns:\n",
      " ['date', 'Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2']\n",
      "\n",
      "Dtypes:\n",
      " date               str\n",
      "Appliances       int64\n",
      "lights           int64\n",
      "T1             float64\n",
      "RH_1           float64\n",
      "T2             float64\n",
      "RH_2           float64\n",
      "T3             float64\n",
      "RH_3           float64\n",
      "T4             float64\n",
      "RH_4           float64\n",
      "T5             float64\n",
      "RH_5           float64\n",
      "T6             float64\n",
      "RH_6           float64\n",
      "T7             float64\n",
      "RH_7           float64\n",
      "T8             float64\n",
      "RH_8           float64\n",
      "T9             float64\n",
      "RH_9           float64\n",
      "T_out          float64\n",
      "Press_mm_hg    float64\n",
      "RH_out         float64\n",
      "Windspeed      float64\n",
      "Visibility     float64\n",
      "Tdewpoint      float64\n",
      "rv1            float64\n",
      "rv2            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read CSV + describe\n",
    "df = pd.read_csv(RAW_URL)\n",
    "# Describing the features in the csv file\n",
    "print(\"Shape (rows, cols):\", df.shape)\n",
    "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b229305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad/unparseable dates: 0\n",
      "\n",
      "Time range: 2016-01-11 17:00:00 -> 2016-05-27 18:00:00\n",
      "% intervals equal to 10min: 100.0\n",
      "Most common time gaps:\n",
      " date\n",
      "0 days 00:10:00    19734\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify timestamp, parse, set index\n",
    "if DATE_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected '{DATE_COL}' column not found.\")\n",
    "\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "bad_dates = int(df[DATE_COL].isna().sum())\n",
    "print(\"\\nBad/unparseable dates:\", bad_dates)\n",
    "\n",
    "# Drop rows with invalid timestamps (if any)\n",
    "df = df.dropna(subset=[DATE_COL])\n",
    "\n",
    "# Sort by time and set as index\n",
    "df = df.sort_values(DATE_COL).set_index(DATE_COL)\n",
    "\n",
    "# Verify 10-minute interval consistency\n",
    "diffs = df.index.to_series().diff().dropna()\n",
    "expected_delta = pd.Timedelta(EXPECTED_FREQ)\n",
    "pct_expected = (diffs == expected_delta).mean() * 100\n",
    "\n",
    "print(\"\\nTime range:\", df.index.min(), \"->\", df.index.max())\n",
    "print(f\"% intervals equal to {EXPECTED_FREQ}:\", round(pct_expected, 2))\n",
    "print(\"Most common time gaps:\\n\", diffs.value_counts().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a16acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows found: 0\n",
      "Duplicate timestamps found: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates (rows and timestamps)\n",
    "dup_rows = int(df.duplicated().sum())\n",
    "dup_timestamps = int(df.index.duplicated().sum())\n",
    "\n",
    "print(\"\\nDuplicate rows found:\", dup_rows)\n",
    "print(\"Duplicate timestamps found:\", dup_timestamps)\n",
    "\n",
    "if dup_rows > 0:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "if dup_timestamps > 0:\n",
    "    df = df[~df.index.duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec97ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing before fill: 0\n"
     ]
    }
   ],
   "source": [
    "# Missing value handling:\n",
    "# - Numeric -> mean\n",
    "# - Categorical -> mode (most frequent)\n",
    "missing_before = df.isna().sum().sum()\n",
    "print(\"\\nMissing before fill:\", int(missing_before))\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Fill numeric NaNs with mean\n",
    "for c in num_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mean())\n",
    "\n",
    "# Fill categorical NaNs with mode\n",
    "for c in cat_cols:\n",
    "    if df[c].isna().any():\n",
    "        df[c] = df[c].fillna(df[c].mode(dropna=True).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "745a747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing timestamps (gaps): 0\n",
      "\n",
      "Missing after fill: 0\n",
      "\n",
      "Cleaning complete. Dataset is time-indexed and contains no missing values.\n"
     ]
    }
   ],
   "source": [
    "# Gap check (missing timestamps)\n",
    "# If gaps exist, document and decide interpolation vs removal\n",
    "full_index = pd.date_range(df.index.min(), df.index.max(), freq=EXPECTED_FREQ)\n",
    "missing_timestamps = full_index.difference(df.index)\n",
    "\n",
    "print(\"\\nMissing timestamps (gaps):\", len(missing_timestamps))\n",
    "\n",
    "# 6) Final check: confirm no NaNs remain\n",
    "missing_after = int(df.isna().sum().sum())\n",
    "print(\"\\nMissing after fill:\", missing_after)\n",
    "\n",
    "assert missing_after == 0, \"There are still missing values after cleaning!\"\n",
    "\n",
    "print(\"\\nCleaning complete. Dataset is time-indexed and contains no missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f088c1",
   "metadata": {},
   "source": [
    "Below code will identify the outliers using the IQR method. If any outliers observed consider for later use as these spikes may be genuine. Any sudden jumps or spikes observed will be later removed after proper analysis.\n",
    "\n",
    "Does feature sanity check to make sure the temp and humidity is in vaid range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfa1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = [\"Appliances\", \"lights\"]\n",
    "RV_COLS = [\"rv1\", \"rv2\"]\n",
    "\n",
    "# time resolution settings\n",
    "EXPECTED_FREQ = \"10min\"\n",
    "SAMPLING_MINUTES = 10\n",
    "\n",
    "# Rolling window chosen to represent 1 day:\n",
    "# 24 hours * 60 minutes = 1440 minutes; 1440/10 = 144 points\n",
    "ROLL_WIN = int((24 * 60) / SAMPLING_MINUTES)\n",
    "\n",
    "# Rolling z-score threshold:\n",
    "# Keeping this as a parameter so it can tune it after inspecting flagged points\n",
    "Z_THRESH = 5\n",
    "\n",
    "# Spike definition for energy: top 5% by default (95th percentile)\n",
    "SPIKE_Q = 0.95\n",
    "\n",
    "# Percentiles used for documentation of extremes (not automatic removal)\n",
    "LOW_P, HIGH_P = 0.01, 0.99\n",
    "\n",
    "# IQR fence multi\n",
    "IQR_K = 3.0\n",
    "\n",
    "# Fetching the columns\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "rv_cols = [c for c in RV_COLS if c in df.columns]\n",
    "sensor_cols = [c for c in num_cols if c not in TARGET_COLS + rv_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f31a2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy spike threshold (q=0.95): 330.00 Wh\n",
      "Number of spike rows: 1001\n"
     ]
    }
   ],
   "source": [
    "# Flag genuine energy spikes (do NOT remove)\n",
    "spike_threshold = df[\"Appliances\"].quantile(SPIKE_Q)\n",
    "df[\"is_energy_spike\"] = df[\"Appliances\"] >= spike_threshold\n",
    "print(f\"Energy spike threshold (q={SPIKE_Q:.2f}): {spike_threshold:.2f} Wh\")\n",
    "print(\"Number of spike rows:\", int(df[\"is_energy_spike\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15012577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity ranges (before correction):\n",
      "Humidity min/max: 1.0 100.0\n",
      "Temp min/max: -6.6 29.8566666666667\n",
      "Pressure min/max: 729.3 772.3\n",
      "\n",
      "NaNs introduced by range sanity checks (sensor glitches): 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks (expected ranges)\n",
    "# - Humidity should be 0–100\n",
    "# - Temperature should be plausible in Celsius\n",
    "# - Weather values should be plausible\n",
    "\n",
    "humidity_cols = [c for c in df.columns if c.startswith(\"RH_\")]\n",
    "if \"RH_out\" in df.columns:\n",
    "    humidity_cols.append(\"RH_out\")\n",
    "\n",
    "temp_cols = [c for c in df.columns if re.match(r\"^T\\d+$\", c)]\n",
    "for c in [\"T_out\", \"Tdewpoint\"]:\n",
    "    if c in df.columns:\n",
    "        temp_cols.append(c)\n",
    "\n",
    "print(\"\\nSanity ranges (before correction):\")\n",
    "print(\"Humidity min/max:\", df[humidity_cols].min().min(), df[humidity_cols].max().max())\n",
    "print(\"Temp min/max:\", df[temp_cols].min().min(), df[temp_cols].max().max())\n",
    "if \"Press_mm_hg\" in df.columns:\n",
    "    print(\"Pressure min/max:\", df[\"Press_mm_hg\"].min(), df[\"Press_mm_hg\"].max())\n",
    "\n",
    "# Apply range rules: anything impossible becomes NaN (sensor glitch)\n",
    "df_range = df.copy()\n",
    "\n",
    "# Humidity: 0–100\n",
    "for c in humidity_cols:\n",
    "    df_range.loc[(df_range[c] < 0) | (df_range[c] > 100), c] = np.nan\n",
    "\n",
    "# Temperatures: broad safe bounds\n",
    "for c in temp_cols:\n",
    "    df_range.loc[(df_range[c] < -30) | (df_range[c] > 60), c] = np.nan\n",
    "\n",
    "# Weather sanity bounds (broad)\n",
    "if \"Press_mm_hg\" in df_range.columns:\n",
    "    df_range.loc[(df_range[\"Press_mm_hg\"] < 600) | (df_range[\"Press_mm_hg\"] > 820), \"Press_mm_hg\"] = np.nan\n",
    "if \"Windspeed\" in df_range.columns:\n",
    "    df_range.loc[(df_range[\"Windspeed\"] < 0) | (df_range[\"Windspeed\"] > 60), \"Windspeed\"] = np.nan\n",
    "if \"Visibility\" in df_range.columns:\n",
    "    df_range.loc[(df_range[\"Visibility\"] < 0) | (df_range[\"Visibility\"] > 100), \"Visibility\"] = np.nan\n",
    "\n",
    "range_nan_added = int(df_range[sensor_cols].isna().sum().sum() - df[sensor_cols].isna().sum().sum())\n",
    "print(\"\\nNaNs introduced by range sanity checks (sensor glitches):\", range_nan_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e0c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top IQR-flagged sensor columns (documentation only):\n",
      "RH_5           542\n",
      "RH_2             9\n",
      "T2               5\n",
      "RH_1             1\n",
      "T1               0\n",
      "T8               0\n",
      "Visibility       0\n",
      "Windspeed        0\n",
      "RH_out           0\n",
      "Press_mm_hg      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Robust extreme-value detection (IQR / percentile / rolling z-score)\n",
    "# We only CORRECT values when rolling z-score indicates a sudden jump\n",
    "\n",
    "def iqr_bounds(series, k=3.0):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return q1 - k * iqr, q3 + k * iqr\n",
    "\n",
    "# IQR flags (for documentation, not automatic removal)\n",
    "iqr_flag_counts = {}\n",
    "for c in sensor_cols:\n",
    "    s = df_range[c].dropna()\n",
    "    if s.empty:\n",
    "        iqr_flag_counts[c] = 0\n",
    "        continue\n",
    "    low, high = iqr_bounds(s, k=3.0)\n",
    "    iqr_flag_counts[c] = int(((df_range[c] < low) | (df_range[c] > high)).sum())\n",
    "\n",
    "iqr_flag_counts = pd.Series(iqr_flag_counts).sort_values(ascending=False)\n",
    "print(\"\\nTop IQR-flagged sensor columns (documentation only):\")\n",
    "print(iqr_flag_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5363d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top rolling z-score corrections (sensor glitches removed):\n",
      "RH_5          163\n",
      "T5             61\n",
      "Visibility     46\n",
      "RH_1           35\n",
      "RH_6           21\n",
      "T3             16\n",
      "RH_3           12\n",
      "T9             10\n",
      "T7              9\n",
      "T4              7\n",
      "dtype: int64\n",
      "Total corrected points (sum across columns): 382\n",
      "\n",
      "Total NaNs after corrections: 0\n"
     ]
    }
   ],
   "source": [
    "# Rolling z-score glitches (this is what we treat as \"sensor glitch\")\n",
    "corrected_counts = {}\n",
    "df_fixed = df_range.copy()\n",
    "\n",
    "for c in sensor_cols:\n",
    "    s = df_fixed[c]\n",
    "    roll_mean = s.rolling(window=ROLL_WIN, min_periods=max(10, ROLL_WIN // 10)).mean()\n",
    "    roll_std  = s.rolling(window=ROLL_WIN, min_periods=max(10, ROLL_WIN // 10)).std()\n",
    "    z = (s - roll_mean) / roll_std.replace(0, np.nan)\n",
    "\n",
    "    glitch_mask = z.abs() > Z_THRESH\n",
    "    corrected_counts[c] = int(glitch_mask.sum())\n",
    "\n",
    "    # Only correct sensor values (NOT energy targets)\n",
    "    df_fixed.loc[glitch_mask, c] = np.nan\n",
    "\n",
    "# Interpolate corrected sensor glitches\n",
    "df_fixed[sensor_cols] = df_fixed[sensor_cols].interpolate(method=\"time\").ffill().bfill()\n",
    "\n",
    "corrected_counts = pd.Series(corrected_counts).sort_values(ascending=False)\n",
    "print(\"\\nTop rolling z-score corrections (sensor glitches removed):\")\n",
    "print(corrected_counts.head(10))\n",
    "print(\"Total corrected points (sum across columns):\", int(corrected_counts.sum()))\n",
    "\n",
    "# Final NaN check\n",
    "nan_total = int(df_fixed.isna().sum().sum())\n",
    "print(\"\\nTotal NaNs after corrections:\", nan_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
